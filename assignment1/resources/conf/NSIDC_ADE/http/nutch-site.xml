<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<!-- Put site-specific property overrides in this file. -->

<configuration>
    <property>
        <name>http.agent.name</name>
        <value>wolegedacao</value>
    </property>

    <property>
        <name>plugin.includes</name>
        <value>protocol-(selenium|httpclient|ftp)|urlfilter-(regex|exactdup|neardup)|parse-(html|tika|metatags|js|zip)|feed|index-(basic|anchor|metadata|more)|indexer-solr|scoring-opic|urlnormalizer-(pass|regex|basic)
        </value>
        <description>Regular expression naming plugin directory names to
            include. Any plugin not matching this expression is excluded.
            In any case you need at least include the nutch-extensionpoints plugin. By
            default Nutch includes crawling just HTML and plain text via HTTP,
            and basic indexing and search plugins. In order to use HTTPS please enable
            protocol-httpclient, but be aware of possible intermittent problems with the
            underlying commons-httpclient library.
        </description>
    </property>

    <!-- mime properties -->
    <property>
        <name>mime.types.file</name>
        <value>tika-mimetypes.xml</value>
        <description>Name of file in CLASSPATH containing filename extension and
            magic sequence to mime types mapping information. Overrides the default Tika config
            if specified.
        </description>
    </property>

    <!-- Used only if plugin parse-metatags is enabled. -->
    <property>
        <name>metatags.names</name>
        <value>description,keywords</value>
        <description>Names of the metatags to extract, separated by ','.
            Use '*' to extract all metatags. Prefixes the names with 'metatag.'
            in the parse-metadata. For instance to index description and keywords,
            you need to activate the plugin index-metadata and set the value of the
            parameter 'index.parse.md' to 'metatag.description,metatag.keywords'.
        </description>
    </property>

    <!-- http properties -->
    <property>
        <name>http.timeout</name>
        <value>30000</value>
        <description>The default network timeout, in milliseconds.</description>
    </property>


    <property>
        <name>http.content.limit</name>
        <value>100000000</value>
        <description>The length limit for downloaded content using the http://
            protocol, in bytes. If this value is nonnegative (>=0), content longer
            than it will be truncated; otherwise, no truncation at all. Do not
            confuse this setting with the file.content.limit setting.
        </description>
    </property>

    <!-- ftp properties -->
    <property>
        <name>ftp.content.limit</name>
        <value>100000000</value>
        <description>The length limit for downloaded content, in bytes.
            If this value is nonnegative (>=0), content longer than it will be truncated;
            otherwise, no truncation at all.
            Caution: classical ftp RFCs never defines partial transfer and, in fact,
            some ftp servers out there do not handle client side forced close-down very
            well. Our implementation tries its best to handle such situations smoothly.
        </description>
    </property>

    <!-- fetcher properties -->
    <property>
        <name>fetcher.server.delay</name>
        <value>5.0</value>
        <description>The number of seconds the fetcher will delay between
            successive requests to the same server. Note that this might get
            overriden by a Crawl-Delay from a robots.txt and is used ONLY if
            fetcher.threads.per.queue is set to 1.
        </description>
    </property>

    <property>
        <name>fetcher.server.min.delay</name>
        <value>5.0</value>
        <description>The minimum number of seconds the fetcher will delay between
            successive requests to the same server. This value is applicable ONLY
            if fetcher.threads.per.queue is greater than 1 (i.e. the host blocking
            is turned off).
        </description>
    </property>

    <property>
        <name>fetcher.max.crawl.delay</name>
        <value>30</value>
        <description>
            If the Crawl-Delay in robots.txt is set to greater than this value (in
            seconds) then the fetcher will skip this page, generating an error report.
            If set to -1 the fetcher will never skip such pages and will wait the
            amount of time retrieved from robots.txt Crawl-Delay, however long that
            might be.
        </description>
    </property>

    <property>
        <name>fetcher.threads.fetch</name>
        <value>10</value>
        <description>The number of FetcherThreads the fetcher should use.
            This is also determines the maximum number of requests that are
            made at once (each FetcherThread handles one connection). The total
            number of threads running in distributed mode will be the number of
            fetcher threads * number of nodes as fetcher has one map task per node.
        </description>
    </property>

    <property>
        <name>fetcher.threads.per.queue</name>
        <value>5</value>
        <description>This number is the maximum number of threads that
            should be allowed to access a queue at one time. Setting it to
            a value > 1 will cause the Crawl-Delay value from robots.txt to
            be ignored and the value of fetcher.server.min.delay to be used
            as a delay between successive requests to the same server instead
            of fetcher.server.delay.
        </description>
    </property>

    <property>
        <name>hadoop.tmp.dir</name>
        <value>/home/renxia/tmp</value>
    </property>

</configuration>
