<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<!-- Put site-specific property overrides in this file. -->

<configuration>
<property>
  <name>http.agent.name</name>
  <value>CSCI572 Doubi Shenmegui</value>
</property>

<property>
  <name>plugin.includes</name>
  <value>protocol-(ftp|http|httpclient)|urlfilter-regex|parse-(html|tika)|index-(basic|anchor)|indexer-solr|scoring-opic|urlnormalizer-(pass|regex|basic)</value>
  <description>Regular expression naming plugin directory names to
   include.  Any plugin not matching this expression is excluded.
   In any case you need at least include the nutch-extensionpoints plugin. By
   default Nutch includes crawling just HTML and plain text via HTTP,
   and basic indexing and search plugins. In order to use HTTPS please enable
   protocol-httpclient, but be aware of possible intermittent problems with the
   underlying commons-httpclient library.
  </description>
 </property>

<!-- http properties -->
<property>
  <name>http.timeout</name>
  <value>30000</value>
  <description>The default network timeout, in milliseconds.</description>
</property>


<property>
  <name>http.content.limit</name>
  <value>10000000</value>
  <description>The length limit for downloaded content using the http://
  protocol, in bytes. If this value is nonnegative (>=0), content longer
  than it will be truncated; otherwise, no truncation at all. Do not
  confuse this setting with the file.content.limit setting.
  </description>
</property>


<property>
  <name>http.redirect.max</name>
  <value>10</value>
  <description>The maximum number of redirects the fetcher will follow when
    trying to fetch a page. If set to negative or 0, fetcher won't immediately
    follow redirected URLs, instead it will record them for later fetching.
  </description>
</property>

<!-- ftp properties -->
<property>
  <name>ftp.keep.connection</name>
  <value>true</value>
  <description>Whether to keep ftp connection. Useful if crawling same host
  again and again. When set to true, it avoids connection, login and dir list
  parser setup for subsequent urls. If it is set to true, however, you must
  make sure (roughly):
  (1) ftp.timeout is less than ftp.server.timeout
  (2) ftp.timeout is larger than (fetcher.threads.fetch * fetcher.server.delay)
  Otherwise there will be too many "delete client because idled too long"
  messages in thread logs.</description>
</property>

<property>
  <name>ftp.content.limit</name>
  <value>10000000</value>
  <description>The length limit for downloaded content, in bytes.
    If this value is nonnegative (>=0), content longer than it will be truncated;
    otherwise, no truncation at all.
    Caution: classical ftp RFCs never defines partial transfer and, in fact,
    some ftp servers out there do not handle client side forced close-down very
    well. Our implementation tries its best to handle such situations smoothly.
  </description>
</property>

<!-- fetcher properties -->
 <property>
  <name>fetcher.server.delay</name>
  <value>2.0</value>
  <description>The number of seconds the fetcher will delay between
   successive requests to the same server. Note that this might get
   overriden by a Crawl-Delay from a robots.txt and is used ONLY if
   fetcher.threads.per.queue is set to 1.
  </description>
 </property>

 <property>
  <name>fetcher.server.min.delay</name>
  <value>5.0</value>
  <description>The minimum number of seconds the fetcher will delay between
   successive requests to the same server. This value is applicable ONLY
   if fetcher.threads.per.queue is greater than 1 (i.e. the host blocking
   is turned off).</description>
 </property>

 <property>
  <name>fetcher.max.crawl.delay</name>
  <value>30</value>
  <description>
   If the Crawl-Delay in robots.txt is set to greater than this value (in
   seconds) then the fetcher will skip this page, generating an error report.
   If set to -1 the fetcher will never skip such pages and will wait the
   amount of time retrieved from robots.txt Crawl-Delay, however long that
   might be.
  </description>
 </property>

 <property>
  <name>fetcher.threads.fetch</name>
  <value>10</value>
  <description>The number of FetcherThreads the fetcher should use.
   This is also determines the maximum number of requests that are
   made at once (each FetcherThread handles one connection). The total
   number of threads running in distributed mode will be the number of
   fetcher threads * number of nodes as fetcher has one map task per node.
  </description>
 </property>

 <property>
  <name>fetcher.threads.per.queue</name>
  <value>1</value>
  <description>This number is the maximum number of threads that
   should be allowed to access a queue at one time. Setting it to
   a value > 1 will cause the Crawl-Delay value from robots.txt to
   be ignored and the value of fetcher.server.min.delay to be used
   as a delay between successive requests to the same server instead
   of fetcher.server.delay.
  </description>
 </property>

 <property>
  <name>fetcher.parse</name>
  <value>false</value>
  <description>If true, fetcher will parse content. Default is false, which means
   that a separate parsing step is required after fetching is finished.</description>
 </property>

<property>
  <name>hadoop.tmp.dir</name>
  <value>/home/renxia/tmp</value>
</property>

</configuration>
